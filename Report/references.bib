
@article{dravins_optical_2013,
	title = {Optical intensity interferometry with the Cherenkov Telescope Array},
	volume = {43},
	issn = {0927-6505},
	url = {https://www.sciencedirect.com/science/article/pii/S0927650512001314},
	doi = {10.1016/j.astropartphys.2012.04.017},
	series = {Seeing the High-Energy Universe with the Cherenkov Telescope Array - The Science Explored with the {CTA}},
	abstract = {With its unprecedented light-collecting area for night-sky observations, the Cherenkov Telescope Array ({CTA}) holds great potential for also optical stellar astronomy, in particular as a multi-element intensity interferometer for realizing imaging with sub-milliarcsecond angular resolution. Such an order-of-magnitude increase of the spatial resolution achieved in optical astronomy will reveal the surfaces of rotationally flattened stars with structures in their circumstellar disks and winds, or the gas flows between close binaries. Image reconstruction is feasible from the second-order coherence of light, measured as the temporal correlations of arrival times between photons recorded in different telescopes. This technique (once pioneered by Hanbury Brown and Twiss) connects telescopes only with electronic signals and is practically insensitive to atmospheric turbulence and to imperfections in telescope optics. Detector and telescope requirements are very similar to those for imaging air Cherenkov observatories, the main difference being the signal processing (calculating cross correlations between single camera pixels in pairs of telescopes). Observations of brighter stars are not limited by sky brightness, permitting efficient {CTA} use during also bright-Moon periods. While other concepts have been proposed to realize kilometer-scale optical interferometers of conventional amplitude (phase-) type, both in space and on the ground, their complexity places them much further into the future than {CTA}, which thus could become the first kilometer-scale optical imager in astronomy.},
	pages = {331--347},
	journaltitle = {Astroparticle Physics},
	shortjournal = {Astroparticle Physics},
	author = {Dravins, Dainis and {LeBohec}, Stephan and Jensen, Hannes and Nuñez, Paul D.},
	urldate = {2023-09-19},
	date = {2013-03-01},
	keywords = {Cherenkov telescopes, Hanbury Brown–Twiss, Intensity interferometry, Optical interferometry, Photon statistics, Stars: individual},
	file = {ScienceDirect Full Text PDF:/home/yuri/Zotero/storage/DME66Z7F/Dravins et al. - 2013 - Optical intensity interferometry with the Cherenko.pdf:application/pdf;ScienceDirect Snapshot:/home/yuri/Zotero/storage/88HR9LKN/S0927650512001314.html:text/html},
}

@article{abdalla_sensitivity_2021,
	title = {Sensitivity of the Cherenkov Telescope Array for probing cosmology and fundamental physics with gamma-ray propagation},
	volume = {2021},
	issn = {1475-7516},
	url = {https://dx.doi.org/10.1088/1475-7516/2021/02/048},
	doi = {10.1088/1475-7516/2021/02/048},
	abstract = {The Cherenkov Telescope Array ({CTA}), the new-generation ground-based observatory for γ astronomy, provides unique capabilities to address significant open questions in astrophysics, cosmology, and fundamental physics. We study some of the salient areas of γ cosmology that can be explored as part of the Key Science Projects of {CTA}, through simulated observations of active galactic nuclei ({AGN}) and of their relativistic jets. Observations of {AGN} with {CTA} will enable a measurement of γ absorption on the extragalactic background light with a statistical uncertainty below 15\% up to a redshift z=2 and to constrain or detect γ halos up to intergalactic-magnetic-field strengths of at least 0.3 {pG} . Extragalactic observations with {CTA} also show promising potential to probe physics beyond the Standard Model. The best limits on Lorentz invariance violation from γ astronomy will be improved by a factor of at least two to three. {CTA} will also probe the parameter space in which axion-like particles could constitute a significant fraction, if not all, of dark matter. We conclude on the synergies between {CTA} and other upcoming facilities that will foster the growth of γ cosmology.},
	pages = {048},
	number = {2},
	journaltitle = {Journal of Cosmology and Astroparticle Physics},
	shortjournal = {J. Cosmol. Astropart. Phys.},
	author = {Abdalla, H. and Abe, H. and Acero, F. and Acharyya, A. and Adam, R. and Agudo, I. and Aguirre-Santaella, A. and Alfaro, R. and Alfaro, J. and Alispach, C. and Aloisio, R. and Batista, R. Alves and Amati, L. and Amato, E. and Ambrosi, G. and Angüner, E. O. and Araudo, A. and Armstrong, T. and Arqueros, F. and Arrabito, L. and Asano, K. and Ascasíbar, Y. and Ashley, M. and Backes, M. and Balazs, C. and Balbo, M. and Balmaverde, B. and Larriva, A. Baquero and Martins, V. Barbosa and Barkov, M. and Baroncelli, L. and Almeida, U. Barres de and Barrio, J. A. and Batista, P.-I. and González, J. Becerra and Becherini, Y. and Beck, G. and Tjus, J. Becker and Belmont, R. and Benbow, W. and Bernardini, E. and Berti, A. and Berton, M. and Bertucci, B. and Beshley, V. and Bi, B. and Biasuzzi, B. and Biland, A. and Bissaldi, E. and Biteau, J. and Blanch, O. and Bocchino, F. and Boisson, C. and Bolmont, J. and Bonanno, G. and Arbeletche, L. Bonneau and Bonnoli, G. and Bordas, P. and Bottacini, E. and Böttcher, M. and Bozhilov, V. and Bregeon, J. and Brill, A. and Brown, A. M. and Bruno, P. and Bruno, A. and Bulgarelli, A. and Burton, M. and Buscemi, M. and Caccianiga, A. and Cameron, R. and Capasso, M. and Caprai, M. and Caproni, A. and Capuzzo-Dolcetta, R. and Caraveo, P. and Carosi, R. and Carosi, A. and Casanova, S. and Cascone, E. and Cauz, D. and Cerny, K. and Cerruti, M. and Chadwick, P. and Chaty, S. and Chen, A. and Chernyakova, M. and Chiaro, G. and Chiavassa, A. and Chytka, L. and Conforti, V. and Conte, F. and Contreras, J. L. and Coronado-Blazquez, J. and Cortina, J. and Costa, A. and Costantini, H. and Covino, S. and Cristofari, P. and Cuevas, O. and D'Ammando, F. and Daniel, M. K. and Davies, J. and Dazzi, F. and Angelis, A. De and Lavergne, M. de Bony de and Caprio, V. De and Anjos, R. de Cássia dos and Pino, E. M. de Gouveia Dal and Lotto, B. De and Martino, D. De and Naurois, M. de and Wilhelmi, E. de Oña and Palma, F. De and Souza, V. de and Delgado, C. and Ceca, R. Della and Volpe, D. della and Depaoli, D. and Girolamo, T. Di and Pierro, F. Di and Díaz, C. and Díaz-Bahamondes, C. and Diebold, S. and Djannati-Ataï, A. and Dmytriiev, A. and Domínguez, A. and Donini, A. and Dorner, D. and Doro, M. and Dournaux, J. and Dwarkadas, V. V. and Ebr, J. and Eckner, C. and Einecke, S. and Ekoume, T. R. N. and Elsässer, D. and Emery, G. and Evoli, C. and Fairbairn, M. and Falceta-Goncalves, D. and Fegan, S. and Feng, Q. and Ferrand, G. and Fiandrini, E. and Fiasson, A. and Fioretti, V. and Foffano, L. and Fonseca, M. V. and Font, L. and Fontaine, G. and Franco, F. J. and Coromina, L. Freixas and Fukami, S. and Fukazawa, Y. and Fukui, Y. and Gaggero, D. and Galanti, G. and Gammaldi, V. and Garcia, E. and Garczarczyk, M. and Gascon, D. and Gaug, M. and Gent, A. and Ghalumyan, A. and Ghirlanda, G. and Gianotti, F. and Giarrusso, M. and Giavitto, G. and Giglietto, N. and Giordano, F. and Glicenstein, J. and Goldoni, P. and González, J. M. and Gourgouliatos, K. and Grabarczyk, T. and Grandi, P. and Granot, J. and Grasso, D. and Green, J. and Grube, J. and Gueta, O. and Gunji, S. and Halim, A. and Harvey, M. and Collado, T. Hassan and Hayashi, K. and Heller, M. and Cadena, S. Hernández and Hervet, O. and Hinton, J. and Hiroshima, N. and Hnatyk, B. and Hnatyk, R. and Hoffmann, D. and Hofmann, W. and Holder, J. and Horan, D. and Hörandel, J. and Horvath, P. and Hovatta, T. and Hrabovsky, M. and Hrupec, D. and Hughes, G. and Hütten, M. and Iarlori, M. and Inada, T. and Inoue, S. and Insolia, A. and Ionica, M. and Iori, M. and Jacquemont, M. and Jamrozy, M. and Janecek, P. and Martínez, I. Jiménez and Jin, W. and Jung-Richardt, I. and Jurysek, J. and Kaaret, P. and Karas, V. and Karkar, S. and Kawanaka, N. and Kerszberg, D. and Khélifi, B. and Kissmann, R. and Knödlseder, J. and Kobayashi, Y. and Kohri, K. and Komin, N. and Kong, A. and Kosack, K. and Kubo, H. and Palombara, N. La and Lamanna, G. and Lang, R. G. and Lapington, J. and Laporte, P. and Lefaucheur, J. and Lemoine-Goumard, M. and Lenain, J. and Leone, F. and Leto, G. and Leuschner, F. and Lindfors, E. and Lloyd, S. and Lohse, T. and Lombardi, S. and Longo, F. and Lopez, A. and López, M. and López-Coto, R. and Loporchio, S. and Lucarelli, F. and Luque-Escamilla, P. L. and Lyard, E. and Maggio, C. and Majczyna, A. and Makariev, M. and Mallamaci, M. and Mandat, D. and Maneva, G. and Manganaro, M. and Manicò, G. and Marcowith, A. and Marculewicz, M. and Markoff, S. and Marquez, P. and Martí, J. and Martinez, O. and Martínez, M. and Martínez, G. and Martínez-Huerta, H. and Maurin, G. and Mazin, D. and Mbarubucyeye, J. D. and Miranda, D. Medina and Meyer, M. and Micanovic, S. and Miener, T. and Minev, M. and Miranda, J. M. and Mitchell, A. and Mizuno, T. and Mode, B. and Moderski, R. and Mohrmann, L. and Molina, E. and Montaruli, T. and Moralejo, A. and Merino, J. Morales and Morcuende-Parrilla, D. and Morselli, A. and Mukherjee, R. and Mundell, C. and Murach, T. and Muraishi, H. and Nagai, A. and Nakamori, T. and Nemmen, R. and Niemiec, J. and Nieto, D. and Nievas, M. and Nikolajuk, M. and Nishijima, K. and Noda, K. and Nosek, D. and Nozaki, S. and O'Brien, P. and Ohira, Y. and Ohishi, M. and Oka, T. and Ong, R. A. and Orienti, M. and Orito, R. and Orlandini, M. and Orlando, E. and Osborne, J. P. and Ostrowski, M. and Oya, I. and Pagliaro, A. and Palatka, M. and Paneque, D. and Pantaleo, F. R. and Paredes, J. M. and Parmiggiani, N. and Patricelli, B. and Pavletić, L. and Pe'er, A. and Pech, M. and Pecimotika, M. and Peresano, M. and Persic, M. and Petruk, O. and Pfrang, K. and Piatteli, P. and Pietropaolo, E. and Pillera, R. and Pilszyk, B. and Pimentel, D. and Pintore, F. and Pita, S. and Pohl, M. and Poireau, V. and Polo, M. and Prado, R. R. and Prast, J. and Principe, G. and Produit, N. and Prokoph, H. and Prouza, M. and Przybilski, H. and Pueschel, E. and Pühlhofer, G. and Pumo, M. L. and Punch, M. and Queiroz, F. and Quirrenbach, A. and Rando, R. and Razzaque, S. and Rebert, E. and Recchia, S. and Reichherzer, P. and Reimer, O. and Reimer, A. and Renier, Y. and Reposeur, T. and Rhode, W. and Ribeiro, D. and Ribó, M. and Richtler, T. and Rico, J. and Rieger, F. and Rizi, V. and Rodriguez, J. and Fernandez, G. Rodriguez and Ramirez, J. C. Rodriguez and Vázquez, J. J. Rodríguez and Romano, P. and Romeo, G. and Roncadelli, M. and Rosado, J. and Leon, A. Rosales de and Rowell, G. and Rudak, B. and Rujopakarn, W. and Russo, F. and Sadeh, I. and Saha, L. and Saito, T. and Greus, F. Salesa and Sanchez, D. and Sánchez-Conde, M. and Sangiorgi, P. and Sano, H. and Santander, M. and Santos, E. M. and Sanuy, A. and Sarkar, S. and Saturni, F. G. and Sawangwit, U. and Scherer, A. and Schleicher, B. and Schovanek, P. and Schussler, F. and Schwanke, U. and Sciacca, E. and Scuderi, S. and Arroyo, M. Seglar and Sergijenko, O. and Servillat, M. and Seweryn, K. and Shalchi, A. and Sharma, P. and Shellard, R. C. and Siejkowski, H. and Sinha, A. and Sliusar, V. and Slowikowska, A. and Sokolenko, A. and Sol, H. and Specovius, A. and Spencer, S. and Spiga, D. and Stamerra, A. and Stanič, S. and Starling, R. and Stolarczyk, T. and Straumann, U. and Strišković, J. and Suda, Y. and Świerk, P. and Tagliaferri, G. and Takahashi, H. and Takahashi, M. and Tavecchio, F. and Taylor, L. and Tejedor, L. A. and Temnikov, P. and Terrier, R. and Terzic, T. and Testa, V. and Tian, W. and Tibaldo, L. and Tonev, D. and Torres, D. F. and Torresi, E. and Tosti, L. and Tothill, N. and Tovmassian, G. and Travnicek, P. and Truzzi, S. and Tuossenel, F. and Umana, G. and Vacula, M. and Vagelli, V. and Valentino, M. and Vallage, B. and Vallania, P. and Eldik, C. van and Varner, G. S. and Vassiliev, V. and Acosta, M. Vázquez and Vecchi, M. and Veh, J. and Vercellone, S. and Vergani, S. and Verguilov, V. and Vettolani, G. P. and Viana, A. and Vigorito, C. F. and Vitale, V. and Vorobiov, S. and Vovk, I. and Vuillaume, T. and Wagner, S. J. and Walter, R. and Watson, J. and White, M. and White, R. and Wiemann, R. and Wierzcholska, A. and Will, M. and Williams, D. A. and Wischnewski, R. and Wolter, A. and Yamazaki, R. and Yanagita, S. and Yang, L. and Yoshikoshi, T. and Zacharias, M. and Zaharijas, G. and Zaric, D. and Zavrtanik, M. and Zavrtanik, D. and Zdziarski, A. A. and Zech, A. and Zechlin, H. and Zhdanov, V. I. and Živec, M.},
	urldate = {2023-09-19},
	date = {2021-02},
	langid = {english},
	file = {IOP Full Text PDF:/home/yuri/Zotero/storage/2IL9FJBN/Abdalla et al. - 2021 - Sensitivity of the Cherenkov Telescope Array for p.pdf:application/pdf},
}

@article{acciari_optical_2020,
	title = {Optical intensity interferometry observations using the {MAGIC} Imaging Atmospheric Cherenkov Telescopes},
	volume = {491},
	issn = {0035-8711},
	url = {https://doi.org/10.1093/mnras/stz3171},
	doi = {10.1093/mnras/stz3171},
	abstract = {Imaging Atmospheric Cherenkov Telescopes ({IACTs}) currently in operation feature large mirrors and order of 1 ns time response to signals of a few photo-electrons produced by optical photons. This means that they are ideally suited for optical interferometry observations. Thanks to their sensitivity to visible wavelengths and long baselines optical intensity interferometry with {IACTs} allows reaching angular resolutions of tens to microarcseconds. We have installed a simple optical setup on top of the cameras of the two 17 m diameter Major Atmospheric Gamma-Ray Imaging Cherenkov ({MAGIC}) {IACTs} and observed coherent fluctuations in the photon intensity measured at the two telescopes for three different stars. The sensitivity is roughly 10 times better than that achieved in the 1970s with the Narrabri interferometer.},
	pages = {1540--1547},
	number = {2},
	journaltitle = {Monthly Notices of the Royal Astronomical Society},
	shortjournal = {Monthly Notices of the Royal Astronomical Society},
	author = {Acciari, V A and Bernardos, M I and Colombo, E and Contreras, J L and Cortina, J and De Angelis, A and Delgado, C and Díaz, C and Fink, D and Mariotti, M and Mangano, S and Mirzoyan, R and Polo, M and Schweizer, T and Will, M},
	urldate = {2023-09-19},
	date = {2020-01-11},
	file = {Full Text PDF:/home/yuri/Zotero/storage/NC6TZ5T2/Acciari et al. - 2020 - Optical intensity interferometry observations usin.pdf:application/pdf},
}

@article{abeysekara_demonstration_2020,
	title = {Demonstration of stellar intensity interferometry with the four {VERITAS} telescopes},
	volume = {4},
	rights = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3366},
	url = {https://www.nature.com/articles/s41550-020-1143-y},
	doi = {10.1038/s41550-020-1143-y},
	abstract = {High angular resolution observations at optical wavelengths provide valuable insights into stellar astrophysics1,2, and enable direct measurements of fundamental stellar parameters3,4 and the probing of stellar atmospheres, circumstellar disks5, the elongation of rapidly rotating stars6 and the pulsations of Cepheid variable stars7. The angular size of most stars is of the order of one milliarcsecond or less, and to spatially resolve stellar disks and features at this scale requires an optical interferometer using an array of telescopes with baselines on the order of hundreds of metres. We report on the implementation of a stellar intensity interferometry system developed for the four {VERITAS} imaging atmospheric Cherenkov telescopes. The system was used to measure the angular diameter of the two sub-milliarcsecond stars β Canis Majoris and ϵ Orionis with a precision of greater than 5\%. The system uses an offline approach in which starlight intensity fluctuations that are recorded at each telescope are correlated post observation. The technique can be readily scaled onto tens to hundreds of telescopes, providing a capability that has proven technically challenging to the current generation of optical amplitude interferometry observatories. This work demonstrates the feasibility of performing astrophysical measurements using imaging atmospheric Cherenkov telescope arrays as intensity interferometers and shows the promise for integrating an intensity interferometry system within future observatories such as the Cherenkov Telescope Array.},
	pages = {1164--1169},
	number = {12},
	journaltitle = {Nature Astronomy},
	shortjournal = {Nat Astron},
	author = {Abeysekara, A. U. and Benbow, W. and Brill, A. and Buckley, J. H. and Christiansen, J. L. and Chromey, A. J. and Daniel, M. K. and Davis, J. and Falcone, A. and Feng, Q. and Finley, J. P. and Fortson, L. and Furniss, A. and Gent, A. and Giuri, C. and Gueta, O. and Hanna, D. and Hassan, T. and Hervet, O. and Holder, J. and Hughes, G. and Humensky, T. B. and Kaaret, P. and Kertzman, M. and Kieda, D. and Krennrich, F. and Kumar, S. and {LeBohec}, T. and Lin, T. T. Y. and Lundy, M. and Maier, G. and Matthews, N. and Moriarty, P. and Mukherjee, R. and Nievas-Rosillo, M. and O’Brien, S. and Ong, R. A. and Otte, A. N. and Pfrang, K. and Pohl, M. and Prado, R. R. and Pueschel, E. and Quinn, J. and Ragan, K. and Reynolds, P. T. and Ribeiro, D. and Richards, G. T. and Roache, E. and Ryan, J. L. and Santander, M. and Sembroski, G. H. and Wakely, S. P. and Weinstein, A. and Wilcox, P. and Williams, D. A. and Williamson, T. J.},
	urldate = {2023-09-19},
	date = {2020-12},
	langid = {english},
	note = {Number: 12
Publisher: Nature Publishing Group},
	keywords = {Astronomical instrumentation, Stars},
	file = {Full Text PDF:/home/yuri/Zotero/storage/BITSDHRA/Abeysekara et al. - 2020 - Demonstration of stellar intensity interferometry .pdf:application/pdf},
}

@article{delgado_intensity_2021,
	title = {Intensity interferometry with the {MAGIC} telescopes},
	volume = {{ICRC}2021},
	doi = {10.22323/1.395.0693},
	pages = {693},
	journaltitle = {{PoS}},
	author = {Delgado, Carlos and {others}},
	date = {2021},
	file = {Full Text PDF:/home/yuri/Zotero/storage/DKJIV3I6/Delgado and others - 2021 - Intensity interferometry with the MAGIC telescopes.pdf:application/pdf},
}

@article{alom_state---art_2019,
	title = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
	volume = {8},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/8/3/292},
	doi = {10.3390/electronics8030292},
	abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning ({DL}), starting with the Deep Neural Network ({DNN}). The survey goes on to cover Convolutional Neural Network ({CNN}), Recurrent Neural Network ({RNN}), including Long Short-Term Memory ({LSTM}) and Gated Recurrent Units ({GRU}), Auto-Encoder ({AE}), Deep Belief Network ({DBN}), Generative Adversarial Network ({GAN}), and Deep Reinforcement Learning ({DRL}). Additionally, we have discussed recent developments, such as advanced variant {DL} techniques based on these {DL} approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, {DL} approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, {SDKs}, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on {DL} using neural networks and a survey on Reinforcement Learning ({RL}). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.},
	pages = {292},
	number = {3},
	journaltitle = {Electronics},
	author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and Van Essen, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
	urldate = {2023-09-19},
	date = {2019-03},
	langid = {english},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {auto-encoder ({AE}), convolutional neural network ({CNN}), deep belief network ({DBN}), deep learning, deep reinforcement learning ({DRL}), generative adversarial network ({GAN}), recurrent neural network ({RNN}), restricted Boltzmann machine ({RBM}), transfer learning},
	file = {Full Text PDF:/home/yuri/Zotero/storage/GLGP4FAX/Alom et al. - 2019 - A State-of-the-Art Survey on Deep Learning Theory .pdf:application/pdf},
}

@article{kim_enhancement_2022,
	title = {Enhancement of Partially Coherent Diffractive Images Using Generative Adversarial Network},
	volume = {3},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2673-2688},
	url = {https://www.mdpi.com/2673-2688/3/2/17},
	doi = {10.3390/ai3020017},
	abstract = {We present a deep learning-based generative model for the enhancement of partially coherent diffractive images. In lensless coherent diffractive imaging, a highly coherent X-ray illumination is required to image an object at high resolution. Non-ideal experimental conditions result in a partially coherent X-ray illumination, lead to imperfections of coherent diffractive images recorded on a detector, and ultimately limit the capability of lensless coherent diffractive imaging. The previous approaches, relying on the coherence property of illumination, require preliminary experiments or expensive computations. In this article, we propose a generative adversarial network ({GAN}) model to enhance the visibility of fringes in partially coherent diffractive images. Unlike previous approaches, the model is trained to restore the latent sharp features from blurred input images without finding coherence properties of illumination. We demonstrate that the {GAN} model performs well with both coherent diffractive imaging and ptychography. It can be applied to a wide range of imaging techniques relying on phase retrieval of coherent diffraction patterns.},
	pages = {274--284},
	number = {2},
	journaltitle = {{AI}},
	author = {Kim, Jong Woo and Messerschmidt, Marc and Graves, William S.},
	urldate = {2023-09-19},
	date = {2022-06},
	langid = {english},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {coherent diffractive imaging, {GAN} (generative adversarial network), partial coherence, phase retrieval, ptychography},
	file = {Full Text PDF:/home/yuri/Zotero/storage/J8N7CP2U/Kim et al. - 2022 - Enhancement of Partially Coherent Diffractive Imag.pdf:application/pdf},
}

@misc{goodfellow_generative_2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	number = {{arXiv}:1406.2661},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2023-09-19},
	date = {2014-06-10},
	eprinttype = {arxiv},
	eprint = {1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/yuri/Zotero/storage/7334ZJ6T/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/yuri/Zotero/storage/Y9EQ2GM2/1406.html:text/html},
}

@misc{isola_image--image_2018,
	title = {Image-to-Image Translation with Conditional Adversarial Networks},
	url = {http://arxiv.org/abs/1611.07004},
	doi = {10.48550/arXiv.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	number = {{arXiv}:1611.07004},
	publisher = {{arXiv}},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	urldate = {2023-09-19},
	date = {2018-11-26},
	eprinttype = {arxiv},
	eprint = {1611.07004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/yuri/Zotero/storage/QIELYDYN/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/home/yuri/Zotero/storage/FHUXVIWE/1611.html:text/html},
}
